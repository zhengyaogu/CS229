{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a set of example $x^{(i)}$, to maximize its likelihood by setting the derivative w.s.t $\\theta$ to 0, we obtain\n",
    "\n",
    "$$\n",
    "\\frac{\\partial l(\\theta)}{\\partial \\theta_j} = \\sum_{i=1}^{m}(y^{(i)} - h_{\\theta}(x^{(i)}))x_{j}^{(i)} = 0\n",
    "$$\n",
    "\n",
    "When $h$ is the logistic function for calculating probability, i.e. $h(x^{(i)}) = \\frac{1}{1 + e^{\\theta^T x^{(i)}}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Written in matrix form,\n",
    "\n",
    "$$X^T(Y - h(X)) = \\boldsymbol{0}$$\n",
    "\n",
    "where $X^T$ is of shape $n \\times m$ ($n$ is the number of features), $Y - h(X)$ is of shape $m \\times 1$. $h(X)$ is a vector of all probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the added intercept terms for each $x^{(i)}$, expand $X^T$, we have\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    " 1 &  \\cdots & 1 \\\\ \n",
    " x_1^{(0)} & \\cdots & x_n^{(0)} \\\\ \n",
    " \\vdots & \\ddots & \\vdots \\\\ \n",
    " x_1^{(m)}& \\cdots & x_n^{(m)}\n",
    "\\end{bmatrix} (Y - h(X)) = \\boldsymbol{0}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the first row of $X^T$ only, we have\n",
    "\n",
    "\\begin{align*}\n",
    "[1, \\cdots, 1](Y - h(X)) &= \\sum_{i=1}^{m}(y^{(i)} - h(x^{(i)})) = 0\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So \n",
    "\n",
    "\\begin{align*}\n",
    "\\sum_{i=1}^{m}y^{(i)} = \\boldsymbol{1} \\{y^{(i)} = 1\\} = \\sum_{i=1}^{m}h(x^{(i)}) = \\sum_{i=1}^{m} P(y^{(i)}=1|x^{(i)};\\theta)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is equivalent to \n",
    "\n",
    "$$\n",
    "\\frac{\\sum_{i \\in I_{0, 1}} P(y^{(i)} = 1 | x^{(i)}; \\theta)}{\\left | \\{i \\in I_{0, 1} \\} \\right |} = \\frac{\\sum_{i \\in I_{0, 1}} \\boldsymbol{1}\\{y^{(i)} = 1\\}}{\\left | \\{i \\in I_{0, 1} \\} \\right |}\n",
    "$$\n",
    "\n",
    "given $(a, b) = (0, 1)$ and all probabilities should be between $(0, 1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (b)\n",
    "\n",
    "1. Perfect calibration doesn't necessarily imply perfect accuracy because caliration is about probability. For example, for a particular $(a, b)$ and a set of test samples, by swapping the predicted probabilities of two samples, it would affect accuracy, but not the calibration.\n",
    "\n",
    "1. Conversely, perfect accuracy shall lead to perfect calibration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When $(a, b) = (0, 1)$, even with $L_2$ regularization, $P(y^{(i)}=1|x^{(i)};\\theta) = \\boldsymbol{1} \\{y^{(i)} = 1\\}$ still holds, so it should change the calibration. \n",
    "\n",
    "However, when $a > 0$, or $b < 1$, imposing $L_2$ regularization will limit the size of $\\theta$, then the predicted probability will tend to be closer to the middle (0.5), hence affecting the calibration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
